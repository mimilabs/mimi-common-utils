{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501b8265-1001-4177-98fb-8fa84b88b2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Dict, Optional, Any\n",
    "import time\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "class ODataAtomParser:\n",
    "    def __init__(self, metadata_xml: Optional[str] = None):\n",
    "        # Define namespaces used in the XML\n",
    "        self.namespaces = {\n",
    "            'a': 'http://www.w3.org/2005/Atom',\n",
    "            'm': 'http://docs.oasis-open.org/odata/ns/metadata',\n",
    "            'd': 'http://docs.oasis-open.org/odata/ns/data'\n",
    "        }\n",
    "        \n",
    "        # Metadata namespaces\n",
    "        self.metadata_namespaces = {\n",
    "            'edmx': 'http://docs.oasis-open.org/odata/ns/edmx',\n",
    "            'edm': 'http://docs.oasis-open.org/odata/ns/edm'\n",
    "        }\n",
    "        \n",
    "        # Parse metadata if provided\n",
    "        self.schema_info = {}\n",
    "        if metadata_xml:\n",
    "            self.parse_metadata(metadata_xml)\n",
    "    \n",
    "    def parse_metadata(self, metadata_xml: str):\n",
    "        \"\"\"Parse EDM metadata to understand data types and schema\"\"\"\n",
    "        try:\n",
    "            root = ET.fromstring(metadata_xml)\n",
    "            \n",
    "            # Find all EntityType definitions\n",
    "            entity_types = root.findall('.//edm:EntityType', self.metadata_namespaces)\n",
    "            \n",
    "            for entity_type in entity_types:\n",
    "                entity_name = entity_type.get('Name')\n",
    "                properties = {}\n",
    "                \n",
    "                # Parse properties\n",
    "                for prop in entity_type.findall('edm:Property', self.metadata_namespaces):\n",
    "                    prop_name = prop.get('Name')\n",
    "                    prop_type = prop.get('Type', 'Edm.String')\n",
    "                    nullable = prop.get('Nullable', 'true').lower() == 'true'\n",
    "                    \n",
    "                    properties[prop_name] = {\n",
    "                        'type': prop_type,\n",
    "                        'nullable': nullable\n",
    "                    }\n",
    "                \n",
    "                self.schema_info[entity_name] = {'properties': properties}\n",
    "                \n",
    "            print(f\"Parsed metadata for {len(self.schema_info)} entity types\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not parse metadata: {e}\")\n",
    "            self.schema_info = {}\n",
    "    \n",
    "    def _add_url_parameter(self, url: str, param: str, value: str) -> str:\n",
    "        \"\"\"Add or update a parameter in the URL\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        query_params = parse_qs(parsed.query)\n",
    "        query_params[param] = [value]\n",
    "        \n",
    "        new_query = urlencode(query_params, doseq=True)\n",
    "        return urlunparse(parsed._replace(query=new_query))\n",
    "    \n",
    "    def fetch_and_parse_url(self, url: str,\n",
    "                           page_size: int = 1000, \n",
    "                           delay_between_requests: float = 0.1):\n",
    "        \"\"\"\n",
    "        Fetch XML from URL and parse it with automatic pagination\n",
    "        \n",
    "        Args:\n",
    "            url: The base URL to fetch data from\n",
    "            entity_name: Optional entity name for schema application\n",
    "            page_size: Number of records per page (default 1000, max 50000 for SODA 2.1)\n",
    "            delay_between_requests: Delay in seconds between API calls to be respectful\n",
    "        \"\"\"\n",
    "        entity_name = url.rstrip('.json').split('/')[-1]\n",
    "        \n",
    "        all_records = []\n",
    "        offset = 0\n",
    "        page_num = 1\n",
    "        \n",
    "        print(f\"Starting to fetch data from: {url}\")\n",
    "        print(f\"Using page size: {page_size}\")\n",
    "        \n",
    "        while True:\n",
    "            # Add pagination parameters to URL\n",
    "            paginated_url = self._add_url_parameter(url, '$limit', str(page_size))\n",
    "            paginated_url = self._add_url_parameter(paginated_url, '$offset', str(offset))\n",
    "            \n",
    "            print(f\"Fetching page {page_num} (offset {offset})...\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(paginated_url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                page_data = response.json()\n",
    "                \n",
    "                # If no data returned, we've reached the end\n",
    "                if not page_data or len(page_data) == 0:\n",
    "                    print(f\"No more data found. Finished at page {page_num - 1}\")\n",
    "                    break\n",
    "                \n",
    "                all_records.extend(page_data)\n",
    "                print(f\"Page {page_num}: Retrieved {len(page_data)} records\")\n",
    "                \n",
    "                # If we got fewer records than requested, we've reached the end\n",
    "                if len(page_data) < page_size:\n",
    "                    print(f\"Retrieved {len(page_data)} records (less than page size). Finished.\")\n",
    "                    break\n",
    "                \n",
    "                # Prepare for next iteration\n",
    "                offset += page_size\n",
    "                page_num += 1\n",
    "                \n",
    "                # Be respectful to the API\n",
    "                if delay_between_requests > 0:\n",
    "                    time.sleep(delay_between_requests)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching page {page_num}: {e}\")\n",
    "                if page_num == 1:\n",
    "                    # If first page fails, re-raise the error\n",
    "                    raise\n",
    "                else:\n",
    "                    # If a later page fails, stop and return what we have\n",
    "                    print(f\"Stopping pagination due to error. Returning {len(all_records)} records.\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Total records retrieved: {len(all_records)}\")\n",
    "        \n",
    "        return self.extract_data(all_records, entity_name)\n",
    "    \n",
    "    def extract_data(self, records, entity_name: Optional[str] = None):\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        return self.apply_schema_types(df, entity_name)\n",
    "    \n",
    "    def _convert_edm_type(self, value: str, edm_type: str):\n",
    "        \"\"\"Convert string value to appropriate Python type based on EDM type\"\"\"\n",
    "        if value is None or value == '':\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            if edm_type == 'Edm.String':\n",
    "                return str(value)\n",
    "            elif edm_type == 'Edm.Int32':\n",
    "                return int(value)\n",
    "            elif edm_type == 'Edm.Int64':\n",
    "                return int(value)\n",
    "            elif edm_type == 'Edm.Decimal':\n",
    "                return float(value)\n",
    "            elif edm_type == 'Edm.Double':\n",
    "                return float(value)\n",
    "            elif edm_type == 'Edm.Boolean':\n",
    "                return value.lower() in ('true', '1', 'yes')\n",
    "            elif edm_type in ('Edm.DateTime', 'Edm.DateTimeOffset'):\n",
    "                return pd.to_datetime(value, errors='coerce')\n",
    "            elif edm_type == 'Edm.Date':\n",
    "                return pd.to_datetime(value, errors='coerce').date()\n",
    "            else:\n",
    "                return None\n",
    "        except (ValueError, AttributeError):\n",
    "            # If conversion fails, return original value\n",
    "            return None\n",
    "    \n",
    "    def apply_schema_types(self, df, entity_name: Optional[str] = None):\n",
    "        \"\"\"Apply schema-based type conversions to DataFrame\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        schema = self.schema_info.get(entity_name) if entity_name else None\n",
    "\n",
    "        if schema:\n",
    "            # Apply type conversions based on schema\n",
    "            for col in df.columns:\n",
    "                if col in schema['properties']:\n",
    "                    prop_info = schema['properties'][col]\n",
    "                    edm_type = prop_info['type']\n",
    "\n",
    "                    \n",
    "                    # Convert entire column\n",
    "                    df[col] = df[col].apply(lambda x: self._convert_edm_type(x, edm_type))\n",
    "            \n",
    "            print(f\"Applied schema-based type conversions for entity '{entity_name}'\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029196e4-106f-40c6-9369-b9b0835b02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, trim\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, FloatType, DoubleType, BooleanType, DateType, TimestampType\n",
    "from typing import Optional\n",
    "\n",
    "def _convert_edm_type_spark(column_name: str, edm_type: str):\n",
    "    \"\"\"Convert column to appropriate Spark type based on EDM type\"\"\"\n",
    "    column = col(column_name)\n",
    "    \n",
    "    if edm_type == 'Edm.String':\n",
    "        return column.cast(StringType())\n",
    "    elif edm_type == 'Edm.Int32':\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(IntegerType()))\n",
    "    elif edm_type == 'Edm.Int64':\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(LongType()))\n",
    "    elif edm_type == 'Edm.Decimal':\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(DoubleType()))\n",
    "    elif edm_type == 'Edm.Double':\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(DoubleType()))\n",
    "    elif edm_type == 'Edm.Boolean':\n",
    "        return when(trim(column) == \"\", None).otherwise(\n",
    "            when(column.rlike(\"(?i)^(true|1|yes)$\"), True).otherwise(False)\n",
    "        ).cast(BooleanType())\n",
    "    elif edm_type in ('Edm.DateTime', 'Edm.DateTimeOffset'):\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(TimestampType()))\n",
    "    elif edm_type == 'Edm.Date':\n",
    "        return when(trim(column) == \"\", None).otherwise(column.cast(DateType()))\n",
    "    else:\n",
    "        return column  # Return original column if type not recognized\n",
    "\n",
    "def apply_schema_types_spark(df: DataFrame, schema_info: dict, entity_name: Optional[str] = None):\n",
    "    \"\"\"Apply schema-based type conversions to Spark DataFrame\"\"\"\n",
    "    if df.count() == 0:  # Check if DataFrame is empty\n",
    "        return df\n",
    "    \n",
    "    schema = schema_info.get(entity_name) if entity_name else None\n",
    "    \n",
    "    if schema:\n",
    "        # Apply type conversions based on schema\n",
    "        for col_name in df.columns:\n",
    "            if col_name in schema['properties']:\n",
    "                prop_info = schema['properties'][col_name]\n",
    "                edm_type = prop_info['type']\n",
    "                \n",
    "                # Apply conversion to the column\n",
    "                converted_col = _convert_edm_type_spark(col_name, edm_type)\n",
    "                df = df.withColumn(col_name, converted_col)\n",
    "        \n",
    "        print(f\"Applied schema-based type conversions for entity '{entity_name}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2264408-d408-4477-be65-d978dcbf7cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "odata_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
